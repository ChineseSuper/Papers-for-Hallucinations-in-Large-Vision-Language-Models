# Papers for Hallucinations in Large Vision-Language Models
We categorize existing papers according to the definition, evaluation, and mitigation of hallucinations in Large Vision-Language Models(LVLMs). **It is worth noting that some papers may exist in multiple categories mentioned above.**
<br></br>

## Definitions of Hallucinations in Vision-Language Models
1. Rohrbach A, Hendricks L A, Burns K, et al. [Object hallucination in image captioning](https://arxiv.org/abs/1809.02156)[J]. arXiv preprint arXiv:1809.02156, 2018.

<br></br>

## Evaluation of Hallucinations in Large Vision-Language Models
1. Rohrbach A, Hendricks L A, Burns K, et al. [Object hallucination in image captioning](https://arxiv.org/abs/1809.02156)[J]. arXiv preprint arXiv:1809.02156, 2018.
2. Li Y, Du Y, Zhou K, et al. [Evaluating object hallucination in large vision-language models](https://arxiv.org/abs/2305.10355)[J]. arXiv preprint arXiv:2305.10355, 2023.
3. Hu H, Zhang J, Zhao M, et al. [CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning](https://arxiv.org/abs/2309.02301)[J]. arXiv preprint arXiv:2309.02301, 2023.
4. Gunjal A, Yin J, Bas E. [Detecting and preventing hallucinations in large vision language models](https://arxiv.org/abs/2308.06394)[J]. arXiv preprint arXiv:2308.06394, 2023.
5. Wang J, Zhou Y, Xu G, et al. [Evaluation and Analysis of Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2308.15126)[J]. arXiv preprint arXiv:2308.15126, 2023.
6. Liu H, Wan X. [Models See Hallucinations: Evaluating the Factuality in Video Captioning](https://arxiv.org/abs/2303.02961)[J]. arXiv preprint arXiv:2303.02961, 2023.
7. Liu F, Lin K, Li L, et al. [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://arxiv.org/abs/2306.14565)[J]. arXiv preprint arXiv:2306.14565, 2023.
8. Lovenia H, Dai W, Cahyawijaya S, et al. [Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models](https://arxiv.org/abs/2310.05338)[J]. arXiv preprint arXiv:2310.05338, 2023.

<br></br>

## Mitigation of Hallucinations in Large Vision-Language Models
1. Biten A F, GÃ³mez L, Karatzas D. [Let there be a clock on the beach: Reducing object hallucination in image captioning](http://openaccess.thecvf.com/content/WACV2022/html/Biten_Let_There_Be_a_Clock_on_the_Beach_Reducing_Object_WACV_2022_paper.html)[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022: 1381-1390. (Not for Large Vision-Language Models, only for image caption models)
2. Dai W, Liu Z, Ji Z, et al. [Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training](https://arxiv.org/abs/2210.07688)[C]//Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023: 2128-2140. (Not for large vision-language models, only for vision-language pretraining models)
3. Hu H, Zhang J, Zhao M, et al. [CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning](https://arxiv.org/abs/2309.02301)[J]. arXiv preprint arXiv:2309.02301, 2023.
4. Gunjal A, Yin J, Bas E. [Detecting and preventing hallucinations in large vision language models](https://arxiv.org/abs/2308.06394)[J]. arXiv preprint arXiv:2308.06394, 2023.
5. Wang B, Wu F, Han X, et al. [VIGC: Visual Instruction Generation and Correction](https://arxiv.org/abs/2308.12714)[J]. arXiv preprint arXiv:2308.12714, 2023.
6. Liu F, Lin K, Li L, et al. [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://arxiv.org/abs/2306.14565)[J]. arXiv preprint arXiv:2306.14565, 2023.
7. Sun Z, Shen S, Cao S, et al. [Aligning Large Multimodal Models with Factually Augmented RLHF](https://arxiv.org/abs/2309.14525)[J]. arXiv preprint arXiv:2309.14525, 2023.
8. You H, Zhang H, Gan Z, et al. [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://arxiv.org/abs/2310.07704)[J]. arXiv preprint arXiv:2310.07704, 2023. 
9. Zhou Y, Cui C, Yoon J, et al. [Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2310.00754)[J]. arXiv preprint arXiv:2310.00754, 2023. (Mitigating hallucination is not the main purpose of this paper)
10. Zhai B, Yang S, Zhao X, et al. [HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption](https://arxiv.org/abs/2310.01779)[J]. arxiv preprint arXiv:2310.01779, 2023.
11. Yin S, Fu C, Zhao S, et al. [Woodpecker: Hallucination Correction for Multimodal Large Language Models](https://arxiv.org/abs/2310.16045)[J]. arXiv preprint arXiv:2310.16045, 2023.

